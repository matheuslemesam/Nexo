"""
Servi√ßo de integra√ß√£o com o Google Gemini AI.
Respons√°vel por gerar an√°lises e insights de projetos.
"""

import json
import asyncio
import httpx
import re
from typing import Optional
from dataclasses import dataclass

from core.config import settings


@dataclass
class GeminiResponse:
    """Resposta do Gemini."""

    content: str
    tokens_used: int
    model: str


class GeminiService:
    """Servi√ßo para interagir com a API do Google Gemini."""

    BASE_URL = "https://generativelanguage.googleapis.com/v1beta"

    # Modelos alternativos para fallback (em ordem de prefer√™ncia)
    # Atualizados para os modelos dispon√≠veis em 2026
    FALLBACK_MODELS = [
        "gemini-2.0-flash-lite",  # Mais leve, menor consumo de quota
        "gemini-2.0-flash",  # Equilibrado
        "gemini-2.5-flash",  # Mais novo
    ]

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or settings.GEMINI_API_KEY
        self.model = settings.GEMINI_MODEL

        if not self.api_key:
            raise ValueError(
                "GEMINI_API_KEY n√£o configurada. "
                "Adicione ao arquivo .env ou passe como par√¢metro."
            )

    async def generate_content(
        self,
        prompt: str,
        system_instruction: Optional[str] = None,
        max_tokens: int = None,
        temperature: float = 0.7,
        max_retries: int = 3,
    ) -> GeminiResponse:
        """
        Gera conte√∫do usando o Gemini com retry autom√°tico.

        Args:
            prompt: O prompt principal para o modelo
            system_instruction: Instru√ß√µes de sistema para guiar o comportamento
            max_tokens: M√°ximo de tokens na resposta
            temperature: Controle de criatividade (0.0 - 1.0)
            max_retries: N√∫mero m√°ximo de tentativas em caso de rate limit

        Returns:
            GeminiResponse com o conte√∫do gerado
        """
        # Lista de modelos para tentar (modelo principal + fallbacks)
        models_to_try = [self.model] + [
            m for m in self.FALLBACK_MODELS if m != self.model
        ]

        last_error = None

        for model in models_to_try:
            try:
                return await self._try_generate(
                    model=model,
                    prompt=prompt,
                    system_instruction=system_instruction,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    max_retries=max_retries,
                )
            except ValueError as e:
                last_error = e
                error_str = str(e)
                # Se for rate limit (429), tenta o pr√≥ximo modelo
                if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str:
                    print(f"‚ö†Ô∏è Modelo {model} com rate limit, tentando pr√≥ximo...")
                    continue
                # Se for outro erro, propaga
                raise

        # Se todos os modelos falharam
        raise ValueError(
            f"Todos os modelos est√£o com rate limit. "
            f"Aguarde alguns minutos e tente novamente. "
            f"√öltimo erro: {last_error}"
        )

    async def _try_generate(
        self,
        model: str,
        prompt: str,
        system_instruction: Optional[str],
        max_tokens: Optional[int],
        temperature: float,
        max_retries: int,
    ) -> GeminiResponse:
        """Tenta gerar conte√∫do com um modelo espec√≠fico."""

        url = f"{self.BASE_URL}/models/{model}:generateContent"

        # Monta o corpo da requisi√ß√£o
        request_body = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens or settings.GEMINI_MAX_TOKENS,
                "topP": 0.95,
                "topK": 40,
            },
        }

        # Adiciona system instruction se fornecida
        if system_instruction:
            request_body["systemInstruction"] = {
                "parts": [{"text": system_instruction}]
            }

        for attempt in range(max_retries):
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    url,
                    params={"key": self.api_key},
                    json=request_body,
                    headers={"Content-Type": "application/json"},
                )

                if response.status_code == 200:
                    data = response.json()

                    # Extrai o conte√∫do da resposta
                    try:
                        content = data["candidates"][0]["content"]["parts"][0]["text"]
                        tokens_used = data.get("usageMetadata", {}).get(
                            "totalTokenCount", 0
                        )
                    except (KeyError, IndexError) as e:
                        raise ValueError(f"Resposta inesperada do Gemini: {str(e)}")

                    return GeminiResponse(
                        content=content, tokens_used=tokens_used, model=model
                    )

                elif response.status_code == 429:
                    # Rate limit - extrai tempo de retry se dispon√≠vel
                    error_text = response.text
                    retry_delay = self._extract_retry_delay(error_text)

                    if attempt < max_retries - 1:
                        wait_time = min(retry_delay or (2**attempt * 5), 60)
                        print(
                            f"‚è≥ Rate limit no modelo {model}. Aguardando {wait_time}s..."
                        )
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        # √öltima tentativa falhou, propaga para tentar outro modelo
                        raise ValueError(f"Rate limit no modelo {model}: {error_text}")

                else:
                    error_detail = response.text
                    raise ValueError(
                        f"Erro na API do Gemini ({response.status_code}): {error_detail}"
                    )

        raise ValueError(f"M√°ximo de tentativas excedido para modelo {model}")

    def _extract_retry_delay(self, error_text: str) -> Optional[int]:
        """Extrai o tempo de retry sugerido da mensagem de erro."""
        try:
            # Procura por padr√µes como "retry in 55.205722526s" ou "retryDelay": "55s"
            match = re.search(r"retry.*?(\d+(?:\.\d+)?)\s*s", error_text, re.IGNORECASE)
            if match:
                return int(float(match.group(1)))
        except:
            pass
        return None


# === Prompts para An√°lise de Projetos ===

SYSTEM_INSTRUCTION_ONBOARDING = """
Voc√™ √© um especialista em an√°lise de c√≥digo e arquitetura de software. 
Sua fun√ß√£o √© analisar reposit√≥rios de c√≥digo e fornecer um relat√≥rio de onboarding 
completo e detalhado para novos desenvolvedores.

Diretrizes:
- Seja objetivo e estruturado
- Use markdown para formata√ß√£o
- Forne√ßa exemplos pr√°ticos quando relevante
- Destaque pontos cr√≠ticos e boas pr√°ticas
- Considere o contexto do projeto (linguagens, frameworks, tamanho)
- Escreva em portugu√™s brasileiro

Formato de sa√≠da esperado:
Use se√ß√µes claras com headers markdown (##, ###)
"""


def build_onboarding_prompt(
    repo_info: dict,
    file_analysis: dict,
    dependencies: list,
    directory_structure: dict,
    code_context: str,
    languages: dict,
) -> str:
    """
    Constr√≥i o prompt para an√°lise de onboarding.

    Args:
        repo_info: Metadados do reposit√≥rio
        file_analysis: Estat√≠sticas de arquivos
        dependencies: Lista de depend√™ncias
        directory_structure: Estrutura de diret√≥rios
        code_context: C√≥digo fonte extra√≠do
        languages: Estat√≠sticas de linguagens

    Returns:
        Prompt formatado para o Gemini
    """
    # Formata informa√ß√µes do reposit√≥rio
    repo_section = "## Informa√ß√µes do Reposit√≥rio\n"
    if repo_info:
        repo_section += f"""
- **Nome:** {repo_info.get("name", "N/A")}
- **Descri√ß√£o:** {repo_info.get("description", "Sem descri√ß√£o")}
- **Linguagem Principal:** {repo_info.get("language", "N/A")}
- **Estrelas:** {repo_info.get("stars", 0)}
- **Forks:** {repo_info.get("forks", 0)}
- **Issues Abertas:** {repo_info.get("open_issues", 0)}
"""
    else:
        repo_section += "Informa√ß√µes n√£o dispon√≠veis.\n"

    # Formata linguagens
    languages_section = "## Linguagens Utilizadas\n"
    if languages:
        total_bytes = sum(languages.values())
        for lang, bytes_count in sorted(
            languages.items(), key=lambda x: x[1], reverse=True
        )[:10]:
            percentage = (bytes_count / total_bytes) * 100 if total_bytes > 0 else 0
            languages_section += f"- **{lang}:** {percentage:.1f}%\n"
    else:
        languages_section += "N√£o foi poss√≠vel determinar as linguagens.\n"

    # Formata an√°lise de arquivos
    files_section = "## Estat√≠sticas de Arquivos\n"
    summary = file_analysis.get("summary", {})
    files_section += f"""
- **Total de Arquivos:** {summary.get("total_files", 0)}
- **Total de Linhas:** {summary.get("total_lines", 0)}
- **Tamanho Total:** {summary.get("total_size", "N/A")}

### Por Categoria:
"""
    for category, stats in file_analysis.get("by_category", {}).items():
        files_section += f"- **{category.title()}:** {stats.get('processed', 0)} arquivos, {stats.get('total_lines', 0)} linhas\n"

    # Formata depend√™ncias
    deps_section = "## Depend√™ncias\n"
    if dependencies:
        for dep in dependencies:
            deps_section += (
                f"\n### {dep.get('manager', 'N/A')} ({dep.get('file', '')})\n"
            )
            deps_section += f"- **Produ√ß√£o:** {', '.join(dep.get('dependencies', [])[:15]) or 'Nenhuma'}\n"
            if dep.get("dev_dependencies"):
                deps_section += f"- **Desenvolvimento:** {', '.join(dep.get('dev_dependencies', [])[:10])}\n"
    else:
        deps_section += "Nenhuma depend√™ncia detectada.\n"

    # Formata estrutura de diret√≥rios (simplificada)
    structure_section = "## Estrutura de Diret√≥rios\n```\n"
    structure_section += json.dumps(directory_structure, indent=2, ensure_ascii=False)[
        :2000
    ]
    structure_section += "\n```\n"

    # Monta o prompt completo
    prompt = f"""
Analise o seguinte projeto de software e gere um relat√≥rio de **Onboarding** completo.

{repo_section}

{languages_section}

{files_section}

{deps_section}

{structure_section}

## C√≥digo Fonte
<code_context>
{code_context[:100000]}
</code_context>

---

## Sua An√°lise Deve Incluir:

### 1. üìã Vis√£o Geral do Projeto
- O que o projeto faz?
- Qual problema ele resolve?
- Quem √© o p√∫blico-alvo?

### 2. üèóÔ∏è Arquitetura e Estrutura
- Como o projeto est√° organizado?
- Quais s√£o os principais m√≥dulos/componentes?
- Explique o fluxo de dados principal

### 3. üõ†Ô∏è Stack Tecnol√≥gica
- Frameworks e bibliotecas principais
- Por que essas escolhas fazem sentido?
- Vers√µes importantes a considerar

### 4. üöÄ Como Come√ßar (Getting Started)
- Pr√©-requisitos para rodar o projeto
- Passos para configura√ß√£o do ambiente
- Comandos principais (build, run, test)

### 5. üìÅ Arquivos Importantes
- Quais arquivos um novo dev deve ler primeiro?
- Onde est√£o as configura√ß√µes principais?
- Pontos de entrada da aplica√ß√£o

### 6. üéØ Padr√µes e Conven√ß√µes
- Padr√µes de c√≥digo identificados
- Conven√ß√µes de nomenclatura
- Estrutura de pastas seguida

### 7. ‚ö†Ô∏è Pontos de Aten√ß√£o
- √Åreas complexas que precisam de cuidado
- D√©bitos t√©cnicos vis√≠veis
- Poss√≠veis melhorias

### 8. üìö Recursos Adicionais
- Documenta√ß√£o recomendada
- Links √∫teis para as tecnologias usadas

Forne√ßa uma an√°lise detalhada, pr√°tica e √∫til para um desenvolvedor que est√° entrando no projeto.
"""

    return prompt


def build_quick_analysis_prompt(
    repo_info: dict, file_analysis: dict, dependencies: list, languages: dict
) -> str:
    """
    Constr√≥i um prompt para an√°lise r√°pida (sem c√≥digo fonte completo).
    √ötil para uma vis√£o geral sem consumir muitos tokens.
    """
    prompt = f"""
Analise as seguintes informa√ß√µes de um reposit√≥rio e forne√ßa uma vis√£o geral r√°pida:

## Reposit√≥rio
- Nome: {repo_info.get("name", "N/A") if repo_info else "N/A"}
- Descri√ß√£o: {repo_info.get("description", "N/A") if repo_info else "N/A"}
- Linguagem: {repo_info.get("language", "N/A") if repo_info else "N/A"}
- Estrelas: {repo_info.get("stars", 0) if repo_info else 0}

## Linguagens
{json.dumps(languages, indent=2) if languages else "N/A"}

## Arquivos
- Total: {file_analysis.get("summary", {}).get("total_files", 0)}
- Linhas: {file_analysis.get("summary", {}).get("total_lines", 0)}

## Depend√™ncias
{json.dumps([d.get("dependencies", [])[:10] for d in dependencies], indent=2) if dependencies else "N/A"}

---

Forne√ßa em portugu√™s:
1. **Tipo de Projeto:** (Web app, API, CLI, Library, etc.)
2. **Stack Principal:** (ex: React + Node.js + MongoDB)
3. **Complexidade Estimada:** (Baixa/M√©dia/Alta)
4. **Resumo em 2-3 frases:** O que este projeto provavelmente faz?
5. **Pr√≥ximos Passos Sugeridos:** O que um dev deveria fazer primeiro?
"""
    return prompt
